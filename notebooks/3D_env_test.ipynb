{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gym in d:\\python\\lib\\site-packages (0.26.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in d:\\python\\lib\\site-packages (from gym) (2.0.0)\n","Requirement already satisfied: numpy>=1.18.0 in d:\\python\\lib\\site-packages (from gym) (1.26.4)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in d:\\python\\lib\\site-packages (from gym) (4.11.3)\n","Requirement already satisfied: gym-notices>=0.0.4 in d:\\python\\lib\\site-packages (from gym) (0.0.8)\n","Requirement already satisfied: zipp>=0.5 in d:\\python\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.7.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import gym\n","from gym import spaces\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["actions = ['up','down','left','right','forward','backward','shoot','rotate clockwise on Z','rotate counterclockwise on Z','rotate clockwise on X','rotate counterclockwise on X', 'rotate clockwise on Y','rotate counter-clockwise on Y']\n","class CustomEnv3D(gym.Env):\n","    def __init__(self):\n","        super(CustomEnv3D, self).__init__()\n","        # Define action and observation spaces\n","        self.grid_size = 1024\n","        self.grid = np.zeros((self.grid_size, self.grid_size, self.grid_size, 3), dtype=np.uint8)\n","        self.action_space = spaces.Discrete(13) \n","\n","        self.laction1 = -1\n","        self.laction2 = -1\n","        # Initialize agent positions and orientations\n","        self.agent1_pos = [512, 512, 512]  # Initial position of agent 1\n","        self.agent2_pos = [256, 256, 256]  # Initial position of agent 2\n","        self.agent1_facing = [0, 0, 1]  # Initial facing direction of agent 1 (towards positive z-axis)\n","        self.agent2_facing = [0, 0, -1]  # Initial facing direction of agent 2 (towards negative z-axis)\n","\n","        self.agent_size = 20\n","\n","        self.reward1 = 0\n","        self.reward2 = 0\n","\n","    def get_direction(self, agentAttacking, agentDefending):\n","        # Calculate direction from agentAttacking to agentDefending\n","        diff = np.array(agentDefending) - np.array(agentAttacking)\n","        return np.degrees(np.arctan2(diff[1], diff[0]))\n","\n","    def step(self, action1, action2):\n","        self.reward1 -= 1\n","        self.reward2 -= 1\n","        self.laction1 = action1\n","        self.laction2 = action2\n","\n","        #Adding premove positions to account for player 1 moving first (so that if P2 shoots they will end up hitting the other agent even if P1 moved first)\n","        self.agent1_prepos = self.agent1_pos\n","\n","        # Agent 1 actions\n","        self._take_action(self.agent1_pos, action1, self.agent1_facing, self.agent2_pos, 1)\n","\n","        # Agent 2 actions\n","        self._take_action(self.agent2_pos, action2, self.agent2_facing, self.agent1_prepos, 2)\n","\n","        # Clip agent positions to stay within the grid boundaries\n","        self.agent1_pos = np.clip(self.agent1_pos, [0, 0, 0], [1023, 1023, 1023])\n","        self.agent2_pos = np.clip(self.agent2_pos, [0, 0, 0], [1023, 1023, 1023])\n","\n","        observation1 = self._get_observation(self.agent1_pos, self.agent2_pos)\n","        observation2 = self._get_observation(self.agent2_pos, self.agent1_pos)\n","        done = self.reward1 >= 1000 or self.reward2 >= 1000\n","\n","        return observation1, observation2, self.reward1, self.reward2, done, {}\n","\n","    def _take_action(self, agent_pos, action, agent_facing, enemy_pos, agent_number):\n","        #Readjusting movement speed and size for the 3D map so agents have an easier time getting to one another\n","        if action == 0:  # Move up\n","            agent_pos[2] -= 7\n","        elif action == 1:  # Move down\n","            agent_pos[2] += 7\n","        elif action == 2:  # Move left\n","            agent_pos[1] -= 7\n","        elif action == 3:  # Move right\n","            agent_pos[1] += 7\n","        elif action == 4:  # Move forward\n","            agent_pos[0] += 7\n","        elif action == 5:  # Move backward\n","            agent_pos[0] -= 7\n","        elif action == 6:  # Shoot\n","            distance2 = self.distance()\n","\n","            if distance2 < 100:  # If distance is less than 100, hit the target\n","                if agent_number == 1:\n","                    self.reward1 += 200  # Reward for agent 2 being hit\n","                else:\n","                    self.reward2 += 200  # Reward for agent 1 being hit\n","            else:\n","                self.reward1 -= 10  # Penalty for missing the target\n","        elif action == 7:  # Rotate clockwise along Z\n","            self.agent2_facing[0] += 10\n","        elif action == 8:  # Rotate counter-clockwise along Z\n","            self.agent2_facing[0] -= 10\n","        elif action == 9:  # Rotate clockwise along X\n","            self.agent2_facing[1] += 10\n","        elif action == 10:  # Rotate counter-clockwise along X\n","            self.agent2_facing[1] -= 10\n","        elif action == 11:  # Rotate clockwise along Y\n","            self.agent2_facing[2] += 10\n","        elif action == 12:  # Rotate counter-clockwise along Y\n","            self.agent2_facing[2] -= 10\n","    def distance(self):\n","        z_diff = self.agent1_pos[0] - self.agent2_pos[0]\n","        x_diff = self.agent1_pos[1] - self.agent2_pos[1]\n","        y_diff = self.agent1_pos[2] - self.agent2_pos[2]\n","        dist = math.sqrt(z_diff**2 + y_diff**2 + x_diff**2)\n","        return dist\n","    def _get_observation(self, agent_pos, enemy_pos):\n","        direction = self.get_direction(agent_pos, enemy_pos)\n","        distance = np.linalg.norm(np.array(enemy_pos) - np.array(agent_pos))\n","        return direction, distance\n","\n","    def reset(self):\n","        self.agent1_pos = [768, 512, 512] \n","        self.agent2_pos = [256, 512, 512]\n","        self.agent1_facing = [0, 0, 1]\n","        self.agent2_facing = [0, 0, -1]\n","        self.reward1 = 0\n","        self.reward2 = 0\n","        return self._get_observation(self.agent1_pos, self.agent2_pos), self._get_observation(self.agent2_pos, self.agent1_pos)\n","\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["import numpy as np\n","import random\n","from collections import deque\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","import gym\n","\n","# Define the neural network architecture\n","class DQN(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 24)\n","        self.fc2 = nn.Linear(24, 24)\n","        self.fc3 = nn.Linear(24, output_size)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","# Define the DQN agent\n","class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=2000)\n","        self.gamma = 0.95    # discount rate\n","        self.epsilon = 1.0  # exploration rate\n","        self.epsilon_min = 0.01\n","        self.epsilon_decay = 0.995\n","        self.model = DQN(state_size, action_size)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n","        self.loss_fn = nn.MSELoss()\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        state = torch.FloatTensor(state).unsqueeze(0)\n","        q_values = self.model(state)\n","        return q_values.argmax().item()\n","\n","    def replay(self, batch_size):\n","        if len(self.memory) < batch_size:\n","            return\n","        minibatch = random.sample(self.memory, batch_size)\n","        for state, action, reward, next_state, done in minibatch:\n","            state = torch.FloatTensor(state).unsqueeze(0)\n","            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n","            target = reward\n","            if not done:\n","                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n","            target_f = self.model(state).squeeze(0).tolist()\n","            target_f[action] = target\n","            target_f = torch.FloatTensor(target_f)\n","            self.optimizer.zero_grad()\n","            loss = self.loss_fn(self.model(state).squeeze(0), target_f)\n","            loss.backward()\n","            self.optimizer.step()\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["episode: 41/1000, score1: 573, score2: 1113, e: 0.81, P1's aggregated rewards: 573, P2's aggregated rewards: 1113\n","episode: 46/1000, score1: 232, score2: 1192, e: 0.79, P1's aggregated rewards: 805, P2's aggregated rewards: 2305\n","episode: 47/1000, score1: 379, score2: 1129, e: 0.79, P1's aggregated rewards: 1184, P2's aggregated rewards: 3434\n","episode: 48/1000, score1: 508, score2: 1078, e: 0.79, P1's aggregated rewards: 1692, P2's aggregated rewards: 4512\n","episode: 57/1000, score1: 1045, score2: 65, e: 0.75, P1's aggregated rewards: 2737, P2's aggregated rewards: 4577\n","episode: 61/1000, score1: 1077, score2: 37, e: 0.74, P1's aggregated rewards: 3814, P2's aggregated rewards: 4614\n","episode: 74/1000, score1: 489, score2: 1029, e: 0.69, P1's aggregated rewards: 4303, P2's aggregated rewards: 5643\n","episode: 86/1000, score1: -209, score2: 1171, e: 0.65, P1's aggregated rewards: 4094, P2's aggregated rewards: 6814\n","episode: 94/1000, score1: -978, score2: 1162, e: 0.62, P1's aggregated rewards: 3116, P2's aggregated rewards: 7976\n","episode: 103/1000, score1: 75, score2: 1035, e: 0.6, P1's aggregated rewards: 3191, P2's aggregated rewards: 9011\n","episode: 104/1000, score1: 1039, score2: 439, e: 0.59, P1's aggregated rewards: 4230, P2's aggregated rewards: 9450\n","episode: 105/1000, score1: 1075, score2: 625, e: 0.59, P1's aggregated rewards: 5305, P2's aggregated rewards: 10075\n","episode: 106/1000, score1: 478, score2: 1188, e: 0.59, P1's aggregated rewards: 5783, P2's aggregated rewards: 11263\n","episode: 108/1000, score1: 281, score2: 1151, e: 0.58, P1's aggregated rewards: 6064, P2's aggregated rewards: 12414\n","episode: 109/1000, score1: -122, score2: 1058, e: 0.58, P1's aggregated rewards: 5942, P2's aggregated rewards: 13472\n","episode: 117/1000, score1: 109, score2: 1049, e: 0.56, P1's aggregated rewards: 6051, P2's aggregated rewards: 14521\n","episode: 133/1000, score1: 48, score2: 1028, e: 0.51, P1's aggregated rewards: 6099, P2's aggregated rewards: 15549\n","episode: 134/1000, score1: 1129, score2: 599, e: 0.51, P1's aggregated rewards: 7228, P2's aggregated rewards: 16148\n","episode: 136/1000, score1: -87, score2: 1033, e: 0.51, P1's aggregated rewards: 7141, P2's aggregated rewards: 17181\n","episode: 139/1000, score1: 503, score2: 1103, e: 0.5, P1's aggregated rewards: 7644, P2's aggregated rewards: 18284\n","episode: 151/1000, score1: -18, score2: 1062, e: 0.47, P1's aggregated rewards: 7626, P2's aggregated rewards: 19346\n","episode: 159/1000, score1: 1031, score2: -99, e: 0.45, P1's aggregated rewards: 8657, P2's aggregated rewards: 19247\n","episode: 160/1000, score1: 1089, score2: 369, e: 0.45, P1's aggregated rewards: 9746, P2's aggregated rewards: 19616\n","episode: 165/1000, score1: 532, score2: 1182, e: 0.44, P1's aggregated rewards: 10278, P2's aggregated rewards: 20798\n","episode: 167/1000, score1: 1094, score2: 944, e: 0.43, P1's aggregated rewards: 11372, P2's aggregated rewards: 21742\n","episode: 200/1000, score1: -594, score2: 1116, e: 0.37, P1's aggregated rewards: 10778, P2's aggregated rewards: 22858\n","episode: 219/1000, score1: 1167, score2: 197, e: 0.33, P1's aggregated rewards: 11945, P2's aggregated rewards: 23055\n","episode: 220/1000, score1: 1089, score2: 939, e: 0.33, P1's aggregated rewards: 13034, P2's aggregated rewards: 23994\n","episode: 221/1000, score1: 1022, score2: 92, e: 0.33, P1's aggregated rewards: 14056, P2's aggregated rewards: 24086\n","episode: 222/1000, score1: 1007, score2: -143, e: 0.33, P1's aggregated rewards: 15063, P2's aggregated rewards: 23943\n","episode: 224/1000, score1: 83, score2: 1133, e: 0.33, P1's aggregated rewards: 15146, P2's aggregated rewards: 25076\n","episode: 225/1000, score1: 611, score2: 1051, e: 0.32, P1's aggregated rewards: 15757, P2's aggregated rewards: 26127\n","episode: 226/1000, score1: 521, score2: 1121, e: 0.32, P1's aggregated rewards: 16278, P2's aggregated rewards: 27248\n","episode: 227/1000, score1: 1083, score2: 523, e: 0.32, P1's aggregated rewards: 17361, P2's aggregated rewards: 27771\n","episode: 228/1000, score1: 1162, score2: 402, e: 0.32, P1's aggregated rewards: 18523, P2's aggregated rewards: 28173\n","episode: 230/1000, score1: 850, score2: 1130, e: 0.32, P1's aggregated rewards: 19373, P2's aggregated rewards: 29303\n","episode: 231/1000, score1: 1118, score2: 358, e: 0.31, P1's aggregated rewards: 20491, P2's aggregated rewards: 29661\n","episode: 235/1000, score1: -17, score2: 1053, e: 0.31, P1's aggregated rewards: 20474, P2's aggregated rewards: 30714\n","episode: 241/1000, score1: 323, score2: 1133, e: 0.3, P1's aggregated rewards: 20797, P2's aggregated rewards: 31847\n","episode: 244/1000, score1: 1001, score2: 221, e: 0.29, P1's aggregated rewards: 21798, P2's aggregated rewards: 32068\n","episode: 245/1000, score1: 692, score2: 1112, e: 0.29, P1's aggregated rewards: 22490, P2's aggregated rewards: 33180\n","episode: 246/1000, score1: 1145, score2: 1145, e: 0.29, P1's aggregated rewards: 23635, P2's aggregated rewards: 34325\n","episode: 247/1000, score1: 1117, score2: 527, e: 0.29, P1's aggregated rewards: 24752, P2's aggregated rewards: 34852\n","episode: 248/1000, score1: 1012, score2: 262, e: 0.29, P1's aggregated rewards: 25764, P2's aggregated rewards: 35114\n","episode: 249/1000, score1: 442, score2: 1102, e: 0.29, P1's aggregated rewards: 26206, P2's aggregated rewards: 36216\n","episode: 250/1000, score1: 1096, score2: 536, e: 0.29, P1's aggregated rewards: 27302, P2's aggregated rewards: 36752\n","episode: 252/1000, score1: 1104, score2: 964, e: 0.28, P1's aggregated rewards: 28406, P2's aggregated rewards: 37716\n","episode: 258/1000, score1: 673, score2: 1083, e: 0.27, P1's aggregated rewards: 29079, P2's aggregated rewards: 38799\n","episode: 259/1000, score1: 494, score2: 1144, e: 0.27, P1's aggregated rewards: 29573, P2's aggregated rewards: 39943\n","episode: 262/1000, score1: 302, score2: 1122, e: 0.27, P1's aggregated rewards: 29875, P2's aggregated rewards: 41065\n","episode: 273/1000, score1: -1330, score2: 1040, e: 0.25, P1's aggregated rewards: 28545, P2's aggregated rewards: 42105\n","episode: 274/1000, score1: 394, score2: 1104, e: 0.25, P1's aggregated rewards: 28939, P2's aggregated rewards: 43209\n","episode: 276/1000, score1: -796, score2: 1104, e: 0.25, P1's aggregated rewards: 28143, P2's aggregated rewards: 44313\n","episode: 277/1000, score1: -619, score2: 1121, e: 0.25, P1's aggregated rewards: 27524, P2's aggregated rewards: 45434\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m     agent1\u001b[38;5;241m.\u001b[39mreplay(batch_size)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent2\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m---> 48\u001b[0m     \u001b[43magent2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n","Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     58\u001b[0m target_f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(target_f)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 60\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), target_f)\n\u001b[0;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Create the custom environment\n","env = CustomEnv3D()\n","\n","# Set the state and action sizes\n","state_size = 2 # Assuming each agent's observation is a tuple of (direction, distance)\n","action_size = 13  # Number of actions available to each agent\n","\n","# Create two DQN agents\n","agent1 = DQNAgent(state_size, action_size)\n","agent2 = DQNAgent(state_size, action_size)\n","\n","batch_size = 32\n","episodes = 1000\n","\n","#Aggregating rewards to report during loop\n","agg_reward1 = 0\n","agg_reward2 = 0\n","\n","for e in range(episodes):\n","    # Reset the environment\n","    state1, state2 = env.reset()\n","\n","    for time in range(500):  # Limiting the episode length\n","        # Agents take actions\n","        action1 = agent1.act(state1)\n","        action2 = agent2.act(state2)\n","        next_state1, next_state2, reward1, reward2, done, _ = env.step(action1, action2)\n","        \n","        # Remember the previous state, action, reward, and next state\n","        agent1.remember(state1, action1, reward1, next_state1, done)\n","        agent2.remember(state2, action2, reward2, next_state2, done)\n","\n","        # Update the current state\n","        state1 = next_state1\n","        state2 = next_state2\n","\n","        if done:\n","            agg_reward1 += reward1\n","            agg_reward2 += reward2\n","            print(\"episode: {}/{}, score1: {}, score2: {}, e: {:.2}, P1's aggregated rewards: {}, P2's aggregated rewards: {}\"\n","                  .format(e, episodes, reward1, reward2, agent1.epsilon, agg_reward1, agg_reward2))\n","            break\n","\n","    # Replay to train the agents\n","    if len(agent1.memory) > batch_size:\n","        agent1.replay(batch_size)\n","    if len(agent2.memory) > batch_size:\n","        agent2.replay(batch_size)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":2}
