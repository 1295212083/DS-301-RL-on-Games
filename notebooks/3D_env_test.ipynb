{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gym in d:\\python\\lib\\site-packages (0.26.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in d:\\python\\lib\\site-packages (from gym) (2.0.0)\n","Requirement already satisfied: numpy>=1.18.0 in d:\\python\\lib\\site-packages (from gym) (1.26.4)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in d:\\python\\lib\\site-packages (from gym) (4.11.3)\n","Requirement already satisfied: gym-notices>=0.0.4 in d:\\python\\lib\\site-packages (from gym) (0.0.8)\n","Requirement already satisfied: zipp>=0.5 in d:\\python\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.7.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import gym\n","from gym import spaces\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["actions = ['up','down','left','right','forward','backward','shoot','rotate clockwise on Z','rotate counterclockwise on Z','rotate clockwise on X','rotate counterclockwise on X', 'rotate clockwise on Y','rotate counter-clockwise on Y']\n","class CustomEnv3D(gym.Env):\n","    def __init__(self):\n","        super(CustomEnv3D, self).__init__()\n","        # Define action and observation spaces\n","        self.grid_size = 1024\n","        self.grid = np.zeros((self.grid_size, self.grid_size, self.grid_size, 3), dtype=np.uint8)\n","        self.action_space = spaces.Discrete(13) \n","\n","        self.laction1 = -1\n","        self.laction2 = -1\n","        # Initialize agent positions and orientations\n","        self.agent1_pos = [512, 512, 512]  # Initial position of agent 1\n","        self.agent2_pos = [256, 256, 256]  # Initial position of agent 2\n","        self.agent1_facing = [0, 0, 1]  # Initial facing direction of agent 1 (towards positive z-axis)\n","        self.agent2_facing = [0, 0, -1]  # Initial facing direction of agent 2 (towards negative z-axis)\n","\n","        self.agent_size = 20\n","\n","        self.reward1 = 0\n","        self.reward2 = 0\n","\n","    def get_direction(self, agentAttacking, agentDefending):\n","        # Calculate direction from agentAttacking to agentDefending\n","        diff = np.array(agentDefending) - np.array(agentAttacking)\n","        return np.degrees(np.arctan2(diff[1], diff[0]))\n","\n","    def step(self, action1, action2):\n","        self.reward1 -= 1\n","        self.reward2 -= 1\n","        self.laction1 = action1\n","        self.laction2 = action2\n","\n","        #Adding premove positions to account for player 1 moving first (so that if P2 shoots they will end up hitting the other agent even if P1 moved first)\n","        self.agent1_prepos = self.agent1_pos\n","\n","        # Agent 1 actions\n","        self._take_action(self.agent1_pos, action1, self.agent1_facing, self.agent2_pos, 1)\n","\n","        # Agent 2 actions\n","        self._take_action(self.agent2_pos, action2, self.agent2_facing, self.agent1_prepos, 2)\n","\n","        # Clip agent positions to stay within the grid boundaries\n","        self.agent1_pos = np.clip(self.agent1_pos, [0, 0, 0], [1023, 1023, 1023])\n","        self.agent2_pos = np.clip(self.agent2_pos, [0, 0, 0], [1023, 1023, 1023])\n","\n","        observation1 = self._get_observation(self.agent1_pos, self.agent2_pos)\n","        observation2 = self._get_observation(self.agent2_pos, self.agent1_pos)\n","        done = self.reward1 >= 1000 or self.reward2 >= 1000\n","\n","        return observation1, observation2, self.reward1, self.reward2, done, {}\n","\n","    def _take_action(self, agent_pos, action, agent_facing, enemy_pos, agent_number):\n","        #Readjusting movement speed and size for the 3D map so agents have an easier time getting to one another\n","        if action == 0:  # Move up\n","            agent_pos[2] -= 7\n","        elif action == 1:  # Move down\n","            agent_pos[2] += 7\n","        elif action == 2:  # Move left\n","            agent_pos[1] -= 7\n","        elif action == 3:  # Move right\n","            agent_pos[1] += 7\n","        elif action == 4:  # Move forward\n","            agent_pos[0] += 7\n","        elif action == 5:  # Move backward\n","            agent_pos[0] -= 7\n","        elif action == 6:  # Shoot\n","            distance2 = self.distance()\n","\n","            if distance2 < 100:  # If distance is less than 100, hit the target\n","                if agent_number == 1:\n","                    self.reward1 += 200  # Reward for agent 2 being hit\n","                else:\n","                    self.reward2 += 200  # Reward for agent 1 being hit\n","            else:\n","                self.reward1 -= 10  # Penalty for missing the target\n","        elif action == 7:  # Rotate clockwise along Z\n","            self.agent2_facing[0] += 10\n","        elif action == 8:  # Rotate counter-clockwise along Z\n","            self.agent2_facing[0] -= 10\n","        elif action == 9:  # Rotate clockwise along X\n","            self.agent2_facing[1] += 10\n","        elif action == 10:  # Rotate counter-clockwise along X\n","            self.agent2_facing[1] -= 10\n","        elif action == 11:  # Rotate clockwise along Y\n","            self.agent2_facing[2] += 10\n","        elif action == 12:  # Rotate counter-clockwise along Y\n","            self.agent2_facing[2] -= 10\n","    def distance(self):\n","        z_diff = self.agent1_pos[0] - self.agent2_pos[0]\n","        x_diff = self.agent1_pos[1] - self.agent2_pos[1]\n","        y_diff = self.agent1_pos[2] - self.agent2_pos[2]\n","        dist = math.sqrt(z_diff**2 + y_diff**2 + x_diff**2)\n","        return dist\n","    def _get_observation(self, agent_pos, enemy_pos):\n","        direction = self.get_direction(agent_pos, enemy_pos)\n","        distance = np.linalg.norm(np.array(enemy_pos) - np.array(agent_pos))\n","        return direction, distance\n","\n","    def reset(self):\n","        self.agent1_pos = [768, 512, 512] \n","        self.agent2_pos = [256, 512, 512]\n","        self.agent1_facing = [0, 0, 1]\n","        self.agent2_facing = [0, 0, -1]\n","        self.reward1 = 0\n","        self.reward2 = 0\n","        return self._get_observation(self.agent1_pos, self.agent2_pos), self._get_observation(self.agent2_pos, self.agent1_pos)\n","\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["import numpy as np\n","import random\n","from collections import deque\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","import gym\n","\n","# Define the neural network architecture\n","class DQN(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 24)\n","        self.fc2 = nn.Linear(24, 24)\n","        self.fc3 = nn.Linear(24, output_size)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","# Define the DQN agent\n","class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=2000)\n","        self.gamma = 0.95    # discount rate\n","        self.epsilon = 1.0  # exploration rate\n","        self.epsilon_min = 0.01\n","        self.epsilon_decay = 0.995\n","        self.model = DQN(state_size, action_size)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n","        self.loss_fn = nn.MSELoss()\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        state = torch.FloatTensor(state).unsqueeze(0)\n","        q_values = self.model(state)\n","        return q_values.argmax().item()\n","\n","    def replay(self, batch_size):\n","        if len(self.memory) < batch_size:\n","            return\n","        minibatch = random.sample(self.memory, batch_size)\n","        for state, action, reward, next_state, done in minibatch:\n","            state = torch.FloatTensor(state).unsqueeze(0)\n","            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n","            target = reward\n","            if not done:\n","                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n","            target_f = self.model(state).squeeze(0).tolist()\n","            target_f[action] = target\n","            target_f = torch.FloatTensor(target_f)\n","            self.optimizer.zero_grad()\n","            loss = self.loss_fn(self.model(state).squeeze(0), target_f)\n","            loss.backward()\n","            self.optimizer.step()\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x2 and 6x24)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Replay to train the agents\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent1\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m---> 40\u001b[0m     \u001b[43magent1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent2\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m     42\u001b[0m     agent2\u001b[38;5;241m.\u001b[39mreplay(batch_size)\n","Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     53\u001b[0m target \u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 55\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     56\u001b[0m target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     57\u001b[0m target_f[action] \u001b[38;5;241m=\u001b[39m target\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 6x24)"]}],"source":["# Create the custom environment\n","env = CustomEnv3D()\n","\n","# Set the state and action sizes\n","state_size = 6 # Assuming each agent's observation is a tuple of (direction, distance)\n","action_size = env.action_space.n  # Number of actions available to each agent\n","\n","# Create two DQN agents\n","agent1 = DQNAgent(state_size, action_size)\n","agent2 = DQNAgent(state_size, action_size)\n","\n","batch_size = 32\n","episodes = 1000\n","\n","for e in range(episodes):\n","    # Reset the environment\n","    state1, state2 = env.reset()\n","\n","    for time in range(500):  # Limiting the episode length\n","        # Agents take actions\n","        action1 = agent1.act(state1)\n","        action2 = agent2.act(state2)\n","        next_state1, next_state2, reward1, reward2, done, _ = env.step(action1, action2)\n","        \n","        # Remember the previous state, action, reward, and next state\n","        agent1.remember(state1, action1, reward1, next_state1, done)\n","        agent2.remember(state2, action2, reward2, next_state2, done)\n","\n","        # Update the current state\n","        state1 = next_state1\n","        state2 = next_state2\n","\n","        if done:\n","            print(\"episode: {}/{}, score1: {}, score2: {}, e: {:.2}\"\n","                  .format(e, episodes, reward1, reward2, agent1.epsilon))\n","            break\n","\n","    # Replay to train the agents\n","    if len(agent1.memory) > batch_size:\n","        agent1.replay(batch_size)\n","    if len(agent2.memory) > batch_size:\n","        agent2.replay(batch_size)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":2}
