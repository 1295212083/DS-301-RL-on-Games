{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gym in d:\\python\\lib\\site-packages (0.26.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in d:\\python\\lib\\site-packages (from gym) (2.0.0)\n","Requirement already satisfied: numpy>=1.18.0 in d:\\python\\lib\\site-packages (from gym) (1.26.4)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in d:\\python\\lib\\site-packages (from gym) (4.11.3)\n","Requirement already satisfied: gym-notices>=0.0.4 in d:\\python\\lib\\site-packages (from gym) (0.0.8)\n","Requirement already satisfied: zipp>=0.5 in d:\\python\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.7.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gym\n","from gym import spaces\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["actions = ['up','down','left','right','forward','backward','shoot','rotate clockwise on Z','rotate counterclockwise on Z','rotate clockwise on X','rotate counterclockwise on X', 'rotate clockwise on Y','rotate counter-clockwise on Y']\n","class CustomEnv3D(gym.Env):\n","    def __init__(self):\n","        super(CustomEnv3D, self).__init__()\n","        # Define action and observation spaces\n","        self.grid_size = 1024\n","        self.grid = np.zeros((self.grid_size, self.grid_size, self.grid_size, 3), dtype=np.uint8)\n","        self.action_space = spaces.Discrete(13) \n","\n","        self.laction1 = -1\n","        self.laction2 = -1\n","        # Initialize agent positions and orientations\n","        self.agent1_pos = [512, 512, 512]  # Initial position of agent 1\n","        self.agent2_pos = [256, 256, 256]  # Initial position of agent 2\n","        self.agent1_facing = [0, 0, 1]  # Initial facing direction of agent 1 (towards positive z-axis)\n","        self.agent2_facing = [0, 0, -1]  # Initial facing direction of agent 2 (towards negative z-axis)\n","\n","        self.agent_size = 20\n","\n","        self.reward1 = 0\n","        self.reward2 = 0\n","\n","    def get_direction(self, agentAttacking, agentDefending):\n","        # Calculate direction from agentAttacking to agentDefending\n","        diff = np.array(agentDefending) - np.array(agentAttacking)\n","        return np.degrees(np.arctan2(diff[1], diff[0]))\n","\n","    def step(self, action1, action2):\n","        self.reward1 -= 1\n","        self.reward2 -= 1\n","        self.laction1 = action1\n","        self.laction2 = action2\n","\n","        #Adding premove positions to account for player 1 moving first (so that if P2 shoots they will end up hitting the other agent even if P1 moved first)\n","        self.agent1_prepos = self.agent1_pos\n","\n","        # Agent 1 actions\n","        self._take_action(self.agent1_pos, action1, self.agent1_facing, self.agent2_pos, 1)\n","\n","        # Agent 2 actions\n","        self._take_action(self.agent2_pos, action2, self.agent2_facing, self.agent1_prepos, 2)\n","\n","        # Clip agent positions to stay within the grid boundaries\n","        self.agent1_pos = np.clip(self.agent1_pos, [0, 0, 0], [1023, 1023, 1023])\n","        self.agent2_pos = np.clip(self.agent2_pos, [0, 0, 0], [1023, 1023, 1023])\n","\n","        observation1 = self._get_observation(self.agent1_pos, self.agent2_pos)\n","        observation2 = self._get_observation(self.agent2_pos, self.agent1_pos)\n","        done = self.reward1 >= 1000 or self.reward2 >= 1000\n","\n","        return observation1, observation2, self.reward1, self.reward2, done, {}\n","\n","    def _take_action(self, agent_pos, action, agent_facing, enemy_pos, agent_number):\n","        #Readjusting movement speed and size for the 3D map so agents have an easier time getting to one another\n","        if action == 0:  # Move up\n","            agent_pos[2] -= 7\n","        elif action == 1:  # Move down\n","            agent_pos[2] += 7\n","        elif action == 2:  # Move left\n","            agent_pos[1] -= 7\n","        elif action == 3:  # Move right\n","            agent_pos[1] += 7\n","        elif action == 4:  # Move forward\n","            agent_pos[0] += 7\n","        elif action == 5:  # Move backward\n","            agent_pos[0] -= 7\n","        elif action == 6:  # Shoot\n","            distancel = self.distance()\n","\n","            if distancel < 200:  # If distance is less than 100, hit the target\n","                if agent_number == 1:\n","                    self.reward1 += 1000  # Reward for agent 1 being hit\n","                else:\n","                    self.reward2 += 1000  # Reward for agent 1 being hit\n","            else:\n","                if agent_number == 1:\n","                    self.reward1 -= 1  # Penalty for missing the target\n","                else:\n","                    self.reward2 -= 1\n","        elif action == 7:  # Rotate clockwise along Z\n","            self.agent2_facing[0] += 10\n","        elif action == 8:  # Rotate counter-clockwise along Z\n","            self.agent2_facing[0] -= 10\n","        elif action == 9:  # Rotate clockwise along X\n","            self.agent2_facing[1] += 10\n","        elif action == 10:  # Rotate counter-clockwise along X\n","            self.agent2_facing[1] -= 10\n","        elif action == 11:  # Rotate clockwise along Y\n","            self.agent2_facing[2] += 10\n","        elif action == 12:  # Rotate counter-clockwise along Y\n","            self.agent2_facing[2] -= 10\n","    def distance(self):\n","        z_diff = self.agent1_pos[0] - self.agent2_pos[0]\n","        x_diff = self.agent1_pos[1] - self.agent2_pos[1]\n","        y_diff = self.agent1_pos[2] - self.agent2_pos[2]\n","        dist = math.sqrt(z_diff**2 + y_diff**2 + x_diff**2)\n","        return dist\n","    def _get_observation(self, agent_pos, enemy_pos):\n","        direction = self.get_direction(agent_pos, enemy_pos)\n","        distance = np.linalg.norm(np.array(enemy_pos) - np.array(agent_pos))\n","        return direction, distance\n","\n","    def reset(self):\n","        self.agent1_pos = [768, 512, 512] \n","        self.agent2_pos = [256, 512, 512]\n","        self.agent1_facing = [0, 0, 1]\n","        self.agent2_facing = [0, 0, -1]\n","        self.reward1 = 0\n","        self.reward2 = 0\n","        return self._get_observation(self.agent1_pos, self.agent2_pos), self._get_observation(self.agent2_pos, self.agent1_pos)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From d:\\Python\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]},{"name":"stderr","output_type":"stream","text":["d:\\Python\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n","  from pandas.core.computation.check import NUMEXPR_INSTALLED\n","d:\\Python\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n","  from pandas.core import (\n"]}],"source":["import numpy as np\n","import random\n","from collections import deque\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","import gym\n","\n","# Define the neural network architecture\n","class DQN(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, output_size)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","# Define the DQN agent\n","class DQNAgent:\n","    def __init__(self, state_size, action_size, gamma = 0.95,epsilon = 3.0,epsilon_min=0.01):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=2000)\n","        self.gamma = gamma   # discount rate\n","        self.epsilon = epsilon  # exploration rate\n","        self.epsilon_min = epsilon_min\n","        self.epsilon_decay = 0.999\n","        self.model = DQN(state_size, action_size)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=0.1)\n","        self.loss_fn = nn.MSELoss()\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        state = torch.FloatTensor(state).unsqueeze(0)\n","        q_values = self.model(state)\n","        return q_values.argmax().item()\n","\n","    def replay(self, batch_size):\n","        if len(self.memory) < batch_size:\n","            return\n","        minibatch = random.sample(self.memory, batch_size)\n","        for state, action, reward, next_state, done in minibatch:\n","            state = torch.FloatTensor(state).unsqueeze(0)\n","            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n","            target = reward\n","            if not done:\n","                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n","            target_f = self.model(state).squeeze(0).tolist()\n","            target_f[action] = target\n","            target_f = torch.FloatTensor(target_f)\n","            self.optimizer.zero_grad()\n","            loss = self.loss_fn(self.model(state).squeeze(0), target_f)\n","            loss.backward()\n","            self.optimizer.step()\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["episode: 0/1000, score1: -518, score2: -521, e: 0.5, P1's aggregated rewards: -518, P2's aggregated rewards: -521\n","episode: 1/1000, score1: -522, score2: -517, e: 0.5, P1's aggregated rewards: -1040, P2's aggregated rewards: -1038\n","episode: 2/1000, score1: -523, score2: -510, e: 0.5, P1's aggregated rewards: -1563, P2's aggregated rewards: -1548\n","episode: 3/1000, score1: -522, score2: -517, e: 0.5, P1's aggregated rewards: -2085, P2's aggregated rewards: -2065\n","episode: 4/1000, score1: -522, score2: -779, e: 0.5, P1's aggregated rewards: -2607, P2's aggregated rewards: -2844\n","episode: 5/1000, score1: -513, score2: -520, e: 0.5, P1's aggregated rewards: -3120, P2's aggregated rewards: -3364\n","episode: 6/1000, score1: -768, score2: -519, e: 0.5, P1's aggregated rewards: -3888, P2's aggregated rewards: -3883\n","episode: 7/1000, score1: -525, score2: -518, e: 0.5, P1's aggregated rewards: -4413, P2's aggregated rewards: -4401\n","episode: 8/1000, score1: -519, score2: -515, e: 0.5, P1's aggregated rewards: -4932, P2's aggregated rewards: -4916\n","episode: 9/1000, score1: -522, score2: -522, e: 0.5, P1's aggregated rewards: -5454, P2's aggregated rewards: -5438\n","episode: 10/1000, score1: -517, score2: -522, e: 0.5, P1's aggregated rewards: -5971, P2's aggregated rewards: -5960\n","episode: 11/1000, score1: -517, score2: -523, e: 0.49, P1's aggregated rewards: -6488, P2's aggregated rewards: -6483\n","episode: 12/1000, score1: -519, score2: -514, e: 0.49, P1's aggregated rewards: -7007, P2's aggregated rewards: -6997\n","episode: 13/1000, score1: -519, score2: -512, e: 0.49, P1's aggregated rewards: -7526, P2's aggregated rewards: -7509\n","episode: 14/1000, score1: 1764, score2: -238, e: 0.49, P1's aggregated rewards: -5762, P2's aggregated rewards: -7747\n","episode: 15/1000, score1: -517, score2: -520, e: 0.49, P1's aggregated rewards: -6279, P2's aggregated rewards: -8267\n","episode: 16/1000, score1: -515, score2: -518, e: 0.49, P1's aggregated rewards: -6794, P2's aggregated rewards: -8785\n","episode: 17/1000, score1: -518, score2: -514, e: 0.49, P1's aggregated rewards: -7312, P2's aggregated rewards: -9299\n","episode: 18/1000, score1: -518, score2: -519, e: 0.49, P1's aggregated rewards: -7830, P2's aggregated rewards: -9818\n","episode: 19/1000, score1: -523, score2: -520, e: 0.49, P1's aggregated rewards: -8353, P2's aggregated rewards: -10338\n","episode: 20/1000, score1: 1879, score2: 881, e: 0.49, P1's aggregated rewards: -6474, P2's aggregated rewards: -9457\n","episode: 21/1000, score1: -516, score2: -772, e: 0.49, P1's aggregated rewards: -6990, P2's aggregated rewards: -10229\n","episode: 22/1000, score1: -519, score2: -761, e: 0.49, P1's aggregated rewards: -7509, P2's aggregated rewards: -10990\n","episode: 23/1000, score1: -525, score2: -522, e: 0.49, P1's aggregated rewards: -8034, P2's aggregated rewards: -11512\n","episode: 24/1000, score1: 1865, score2: -93, e: 0.49, P1's aggregated rewards: -6169, P2's aggregated rewards: -11605\n","episode: 25/1000, score1: -517, score2: -518, e: 0.49, P1's aggregated rewards: -6686, P2's aggregated rewards: -12123\n","episode: 26/1000, score1: -513, score2: -521, e: 0.49, P1's aggregated rewards: -7199, P2's aggregated rewards: -12644\n","episode: 27/1000, score1: -515, score2: -517, e: 0.49, P1's aggregated rewards: -7714, P2's aggregated rewards: -13161\n","episode: 28/1000, score1: -520, score2: -518, e: 0.49, P1's aggregated rewards: -8234, P2's aggregated rewards: -13679\n","episode: 29/1000, score1: -521, score2: -517, e: 0.49, P1's aggregated rewards: -8755, P2's aggregated rewards: -14196\n","episode: 30/1000, score1: -526, score2: -521, e: 0.49, P1's aggregated rewards: -9281, P2's aggregated rewards: -14717\n","episode: 31/1000, score1: -512, score2: -518, e: 0.48, P1's aggregated rewards: -9793, P2's aggregated rewards: -15235\n","episode: 32/1000, score1: -518, score2: -520, e: 0.48, P1's aggregated rewards: -10311, P2's aggregated rewards: -15755\n","episode: 33/1000, score1: -518, score2: -516, e: 0.48, P1's aggregated rewards: -10829, P2's aggregated rewards: -16271\n","episode: 34/1000, score1: -517, score2: -521, e: 0.48, P1's aggregated rewards: -11346, P2's aggregated rewards: -16792\n","episode: 35/1000, score1: -515, score2: -785, e: 0.48, P1's aggregated rewards: -11861, P2's aggregated rewards: -17577\n","episode: 36/1000, score1: -512, score2: -516, e: 0.48, P1's aggregated rewards: -12373, P2's aggregated rewards: -18093\n","episode: 37/1000, score1: -518, score2: -518, e: 0.48, P1's aggregated rewards: -12891, P2's aggregated rewards: -18611\n","episode: 38/1000, score1: -514, score2: -525, e: 0.48, P1's aggregated rewards: -13405, P2's aggregated rewards: -19136\n","episode: 39/1000, score1: -521, score2: -522, e: 0.48, P1's aggregated rewards: -13926, P2's aggregated rewards: -19658\n","episode: 40/1000, score1: -513, score2: -516, e: 0.48, P1's aggregated rewards: -14439, P2's aggregated rewards: -20174\n","episode: 41/1000, score1: 1859, score2: 861, e: 0.48, P1's aggregated rewards: -12580, P2's aggregated rewards: -19313\n","episode: 42/1000, score1: -522, score2: -521, e: 0.48, P1's aggregated rewards: -13102, P2's aggregated rewards: -19834\n","episode: 43/1000, score1: -513, score2: -519, e: 0.48, P1's aggregated rewards: -13615, P2's aggregated rewards: -20353\n","episode: 44/1000, score1: -515, score2: -516, e: 0.48, P1's aggregated rewards: -14130, P2's aggregated rewards: -20869\n","episode: 45/1000, score1: -520, score2: -514, e: 0.48, P1's aggregated rewards: -14650, P2's aggregated rewards: -21383\n","episode: 46/1000, score1: -518, score2: -516, e: 0.48, P1's aggregated rewards: -15168, P2's aggregated rewards: -21899\n","episode: 47/1000, score1: -520, score2: -520, e: 0.48, P1's aggregated rewards: -15688, P2's aggregated rewards: -22419\n","episode: 48/1000, score1: -517, score2: -518, e: 0.48, P1's aggregated rewards: -16205, P2's aggregated rewards: -22937\n","episode: 49/1000, score1: -518, score2: -525, e: 0.48, P1's aggregated rewards: -16723, P2's aggregated rewards: -23462\n","episode: 50/1000, score1: -523, score2: -516, e: 0.48, P1's aggregated rewards: -17246, P2's aggregated rewards: -23978\n","episode: 51/1000, score1: -519, score2: -516, e: 0.48, P1's aggregated rewards: -17765, P2's aggregated rewards: -24494\n"]}],"source":["import tensorflow as tf\n","# Create the custom environment\n","env = CustomEnv3D()\n","\n","# Set the state and action sizes\n","state_size = 2 # Assuming each agent's observation is a tuple of (direction, distance)\n","action_size = 13  # Number of actions available to each agent\n","epsilons = [0.5, 1.0, 2.0, 3.0]\n","gammas = [0.95, 0.99]\n","\n","#Saving lists of lists of data to report later from each loop\n","agg_rewards1 = []\n","agg_rewards2 = []\n","scores1 = []\n","scores2 = []\n","epsilon_changes = []\n","\n","target_device = \"/device:GPU:0\"\n","\n","with tf.device(target_device):\n","    # Create two DQN agents\n","    for epsilon in epsilons:\n","        for gamma in gammas:\n","            agent1 = DQNAgent(state_size, action_size, gamma= gamma, epsilon=epsilon)\n","            agent2 = DQNAgent(state_size, action_size, gamma= gamma, epsilon=epsilon)\n","\n","            #Some hyperparameters for the training loop\n","            batch_size = 32\n","            episodes = 1000\n","            num_steps = 500\n","\n","            #Aggregating rewards to report during loop\n","            agg_reward1 = 0\n","            agg_reward2 = 0\n","\n","            for e in range(episodes):\n","                temp_scores1 = []\n","                temp_scores2 = []\n","                temp_agg_reward1 = []\n","                temp_agg_reward2 = []\n","                temp_epsilon_changes = []\n","                # Reset the environment\n","                state1, state2 = env.reset()\n","\n","                for time in range(num_steps):  # Limiting the episode length\n","                    # Agents take actions\n","                    action1 = agent1.act(state1)\n","                    action2 = agent2.act(state2)\n","                    next_state1, next_state2, reward1, reward2, done, _ = env.step(action1, action2)\n","                \n","                    # Remember the previous state, action, reward, and next state\n","                    agent1.remember(state1, action1, reward1, next_state1, done)\n","                    agent2.remember(state2, action2, reward2, next_state2, done)\n","                    \n","                    # Update the current state\n","                    state1 = next_state1\n","                    state2 = next_state2\n","\n","                    if done or time == num_steps - 1:\n","                        agg_reward1 += reward1\n","                        agg_reward2 += reward2\n","                        temp_scores1.append(reward1)\n","                        temp_scores2.append(reward2)\n","                        temp_agg_reward1.append(agg_reward1)\n","                        temp_agg_reward2.append(agg_reward2)\n","                        temp_epsilon_changes.append(epsilon)\n","                        print(\"episode: {}/{}, score1: {}, score2: {}, e: {:.2}, P1's aggregated rewards: {}, P2's aggregated rewards: {}\"\n","                            .format(e, episodes, reward1, reward2, agent1.epsilon, agg_reward1, agg_reward2))\n","                        break\n","\n","                # Replay to train the agents\n","                if len(agent1.memory) > batch_size:\n","                    agent1.replay(batch_size)\n","                if len(agent2.memory) > batch_size:\n","                    agent2.replay(batch_size)\n","            agg_rewards1.append(temp_agg_reward1)\n","            agg_rewards2.append(temp_agg_reward2)\n","            scores1.append(temp_scores1)\n","            scores2.append(temp_scores2)\n","            epsilon_changes.append(temp_epsilon_changes)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":2}
